{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1750841614805,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "qMkNHM0g5Qke",
    "outputId": "8db0a19a-4c27-4e71-cec7-eb961e9eeb73"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install --upgrade pip\n",
    "# !pip install empatches\n",
    "# !pip install tensorflow\n",
    "# !pip install torch\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1750841614839,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "kmAdfc8GSnzY"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from empatches import EMPatches\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from random import random\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import traceback\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1750841615821,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "yQfrshqK120J"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_FOLDER_PATH = '/home/jtelintelo/BadPatches/'\n",
    "os.makedirs(BASE_FOLDER_PATH + '/data_files', exist_ok=True)\n",
    "os.makedirs(BASE_FOLDER_PATH + '/model_files', exist_ok=True)\n",
    "os.makedirs(BASE_FOLDER_PATH + '/result_images', exist_ok=True)\n",
    "\n",
    "trigger = 'square'  # 'square' , 'blend' , 'warped'\n",
    "patch_level = False  # False for image-level trigger\n",
    "\n",
    "if patch_level:\n",
    "    poisoning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.02, 0.06, 0.1]\n",
    "else:\n",
    "    poisoning_rates = [0.001, 0.005, 0.02, 0.06, 0.1]\n",
    "\n",
    "runs = 1  # How many times do you want to train the model with the configuration\n",
    "patch_size = 4  # Hyperparameter 'l'\n",
    "badpatches_patch_size = 4  # Size of the patch to which the trigger is applied in the case of BadPatches\n",
    "square_trigger_size = 2  # Size of trigger patches, remember that black square should be smaller than patch size for patch level\n",
    "\n",
    "train = True  # False if you already trained a model and just want to run validation, not that poisoning of images can be random due to shuffling, results might slightly differ from the training validation\n",
    "\n",
    "training_epochs = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-ycU9O-5Qkg"
   },
   "source": [
    "# Gating Routers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1750841615830,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "cHgmRT3H5Qkg"
   },
   "outputs": [],
   "source": [
    "class gate(tf.keras.layers.Layer):\n",
    "    def __init__(self, k, gating_kernel_size, strides=(1, 1), padding='valid',\n",
    "                 data_format='channels_last', gating_activation=None,\n",
    "                 gating_kernel_initializer=tf.keras.initializers.RandomNormal, **kwargs):\n",
    "        super(gate, self).__init__(**kwargs)\n",
    "        self.k = k\n",
    "        self.gating_kernel_size = gating_kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.data_format = data_format\n",
    "        self.gating_activation = tf.keras.activations.get(gating_activation)\n",
    "        self.gating_kernel_initializer = gating_kernel_initializer\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs should be defined. Found `None`.')\n",
    "\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        gating_kernel_shape = self.gating_kernel_size + (input_dim, 1)\n",
    "        self.gating_kernel = self.add_weight(shape=gating_kernel_shape, initializer=self.gating_kernel_initializer, name='gating_kernel')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        gating_outputs = tf.keras.backend.conv2d(inputs, self.gating_kernel, strides=self.strides,\n",
    "                                                 padding=self.padding, data_format=self.data_format)\n",
    "\n",
    "        gating_outputs = tf.transpose(gating_outputs, perm=(0, 3, 1, 2))\n",
    "        x = tf.shape(gating_outputs)[2]\n",
    "        y = tf.shape(gating_outputs)[3]\n",
    "        gating_outputs = tf.reshape(gating_outputs, (tf.shape(gating_outputs)[0], tf.shape(gating_outputs)[1], x * y))\n",
    "\n",
    "        gating_outputs = self.gating_activation(gating_outputs)\n",
    "        [values, indices] = tf.math.top_k(gating_outputs, k=self.k, sorted=False)\n",
    "        indices = tf.reshape(indices, (tf.shape(indices)[0] * tf.shape(indices)[1], tf.shape(indices)[2]))\n",
    "        values = tf.reshape(values, (tf.shape(values)[0] * tf.shape(values)[1], tf.shape(values)[2]))\n",
    "        batch_t, k_t = tf.unstack(tf.shape(indices), num=2)\n",
    "\n",
    "        n = tf.shape(gating_outputs)[2]\n",
    "\n",
    "        indices_flat = tf.reshape(indices, [-1]) + tf.math.floordiv(tf.range(batch_t * k_t), k_t) * n\n",
    "        ret_flat = tf.math.unsorted_segment_sum(tf.reshape(values, [-1]), indices_flat, batch_t * n)\n",
    "        ret_rsh = tf.reshape(ret_flat, [batch_t, n])\n",
    "        ret_rsh_3 = tf.reshape(ret_rsh, (tf.shape(gating_outputs)[0], tf.shape(gating_outputs)[1], tf.shape(gating_outputs)[2]))\n",
    "\n",
    "        new_gating_outputs = tf.reshape(ret_rsh_3, (tf.shape(ret_rsh_3)[0], tf.shape(ret_rsh_3)[1], x, y))\n",
    "        new_gating_outputs = tf.transpose(new_gating_outputs, perm=(0, 2, 3, 1))\n",
    "        new_gating_outputs = tf.repeat(new_gating_outputs, tf.shape(self.gating_kernel)[0] * tf.shape(self.gating_kernel)[1] * tf.shape(self.gating_kernel)[2], axis=3)\n",
    "        new_gating_outputs = tf.reshape(new_gating_outputs, (tf.shape(new_gating_outputs)[0], tf.shape(new_gating_outputs)[1], tf.shape(new_gating_outputs)[2], tf.shape(self.gating_kernel)[0], tf.shape(self.gating_kernel)[1], tf.shape(self.gating_kernel)[2]))\n",
    "        new_gating_outputs = tf.transpose(new_gating_outputs, perm=(0, 1, 3, 2, 4, 5))\n",
    "        new_gating_outputs = tf.reshape(new_gating_outputs, (tf.shape(new_gating_outputs)[0], tf.shape(new_gating_outputs)[1] * tf.shape(new_gating_outputs)[2], tf.shape(new_gating_outputs)[3] * tf.shape(new_gating_outputs)[4], tf.shape(new_gating_outputs)[5]))\n",
    "        outputs = inputs * new_gating_outputs\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SERp0GMl5Qkh"
   },
   "source": [
    "# Wideresnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1750841615839,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "qMoWs8YN5Qkh"
   },
   "outputs": [],
   "source": [
    "initializer_gate = keras.initializers.RandomNormal(mean=0.0, stddev=0.0001)\n",
    "\n",
    "def WideResnetBlock(x, channels, strides, channel_mismatch=False):\n",
    "    identity = x\n",
    "\n",
    "    out = layers.BatchNormalization()(x)\n",
    "    out = layers.ReLU()(out)\n",
    "    out = layers.Conv2D(filters=channels, kernel_size=3, strides=strides, padding='same')(out)\n",
    "\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.ReLU()(out)\n",
    "    out = layers.Conv2D(filters=channels, kernel_size=3, strides=1, padding='same')(out)\n",
    "\n",
    "    if channel_mismatch is not False:\n",
    "        identity = layers.Conv2D(\n",
    "            filters=channels, kernel_size=1, strides=strides, padding='valid')(identity)\n",
    "\n",
    "    out = layers.Add()([identity, out])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def WideResnetGroup(x, num_blocks, channels, strides):\n",
    "    x = WideResnetBlock(x=x, channels=channels, strides=strides, channel_mismatch=True)\n",
    "\n",
    "    for _ in range(num_blocks - 1):\n",
    "        x = WideResnetBlock(x=x, channels=channels, strides=(1, 1))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def WideResnet(x, num_blocks, k, num_classes=10):\n",
    "    widths = [int(v * k) for v in (16, 32, 64)]\n",
    "\n",
    "    x = layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = WideResnetGroup(x, num_blocks, widths[0], strides=(1, 1))\n",
    "    x = WideResnetGroup(x, num_blocks, widths[1], strides=(2, 2))\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=640, kernel_size=3, strides=2, padding='same')(x)\n",
    "\n",
    "    x_1 = gate(2, (4, 4), (4, 4), gating_activation=tf.nn.softmax, gating_kernel_initializer=initializer_gate)(x)\n",
    "    x_2 = gate(2, (4, 4), (4, 4), gating_activation=tf.nn.softmax, gating_kernel_initializer=initializer_gate)(x)\n",
    "    x_3 = gate(2, (4, 4), (4, 4), gating_activation=tf.nn.softmax, gating_kernel_initializer=initializer_gate)(x)\n",
    "    x_4 = gate(2, (4, 4), (4, 4), gating_activation=tf.nn.softmax, gating_kernel_initializer=initializer_gate)(x)\n",
    "    x_5 = gate(2, (4, 4), (4, 4), gating_activation=tf.nn.softmax, gating_kernel_initializer=initializer_gate)(x)\n",
    "    x_6 = gate(2, (4, 4), (4, 4), gating_activation=tf.nn.softmax, gating_kernel_initializer=initializer_gate)(x)\n",
    "    x_7 = gate(2, (4, 4), (4, 4), gating_activation=tf.nn.softmax, gating_kernel_initializer=initializer_gate)(x)\n",
    "    x_8 = gate(2, (4, 4), (4, 4), gating_activation=tf.nn.softmax, gating_kernel_initializer=initializer_gate)(x)\n",
    "\n",
    "    x_1 = layers.BatchNormalization()(x_1)\n",
    "    x_2 = layers.BatchNormalization()(x_2)\n",
    "    x_3 = layers.BatchNormalization()(x_3)\n",
    "    x_4 = layers.BatchNormalization()(x_4)\n",
    "    x_5 = layers.BatchNormalization()(x_5)\n",
    "    x_6 = layers.BatchNormalization()(x_6)\n",
    "    x_7 = layers.BatchNormalization()(x_7)\n",
    "    x_8 = layers.BatchNormalization()(x_8)\n",
    "\n",
    "    x_1 = layers.ReLU()(x_1)\n",
    "    x_2 = layers.ReLU()(x_2)\n",
    "    x_3 = layers.ReLU()(x_3)\n",
    "    x_4 = layers.ReLU()(x_4)\n",
    "    x_5 = layers.ReLU()(x_5)\n",
    "    x_6 = layers.ReLU()(x_6)\n",
    "    x_7 = layers.ReLU()(x_7)\n",
    "    x_8 = layers.ReLU()(x_8)\n",
    "\n",
    "    x_1 = layers.Conv2D(filters=80, kernel_size=3, strides=1, padding='same')(x_1)\n",
    "    x_2 = layers.Conv2D(filters=80, kernel_size=3, strides=1, padding='same')(x_2)\n",
    "    x_3 = layers.Conv2D(filters=80, kernel_size=3, strides=1, padding='same')(x_3)\n",
    "    x_4 = layers.Conv2D(filters=80, kernel_size=3, strides=1, padding='same')(x_4)\n",
    "    x_5 = layers.Conv2D(filters=80, kernel_size=3, strides=1, padding='same')(x_5)\n",
    "    x_6 = layers.Conv2D(filters=80, kernel_size=3, strides=1, padding='same')(x_6)\n",
    "    x_7 = layers.Conv2D(filters=80, kernel_size=3, strides=1, padding='same')(x_7)\n",
    "    x_8 = layers.Conv2D(filters=80, kernel_size=3, strides=1, padding='same')(x_8)\n",
    "\n",
    "    x = tf.keras.layers.concatenate([x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8])\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.AveragePooling2D((16, 16))(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    x = layers.Dense(units=num_classes, activation='softmax')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-d9cWBl5Qki"
   },
   "source": [
    "# Trigger generation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateSquareTrigger:\n",
    "    \"\"\"\n",
    "    A class that creates a random square pattern that is used as a trigger for an\n",
    "    image dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.dims = (64, 64, 3)\n",
    "        self.size = size\n",
    "        trigger = np.zeros(self.dims, dtype=np.float32)\n",
    "        self.crafted_trigger = self.create_trigger_square(trigger)\n",
    "\n",
    "        if size[0] > self.dims[0] or size[1] > self.dims[1]:\n",
    "            raise Exception(\n",
    "                \"The size of the trigger is too large for the dataset items.\")\n",
    "\n",
    "    def create_trigger_square(self, trigger):\n",
    "        \"\"\"Create a square trigger.\"\"\"\n",
    "        \n",
    "        base_x, base_y = (0, 0)\n",
    "        \n",
    "        for x in range(self.size[0]):\n",
    "            for y in range(self.size[1]):\n",
    "                trigger[base_x + x][base_y + y] = np.zeros((self.dims[2]))\n",
    "\n",
    "        return trigger\n",
    "\n",
    "    def apply_trigger(self, img):\n",
    "        \"\"\"applies the trigger on the image.\"\"\"\n",
    "        \n",
    "        base_x, base_y = (0, 0)\n",
    "        \n",
    "        for x in range(self.size[0]):\n",
    "            for y in range(self.size[1]):\n",
    "                img[base_x + x][base_y + y] = self.crafted_trigger[base_x + x][base_y + y]\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateBlendedTrigger:\n",
    "    \"\"\"\n",
    "    A class that uses images of the same dimensions as the dataset as triggers\n",
    "    that will be blended with the clean images.\n",
    "\n",
    "    We will use a random pattern or a hello-kitty image as the original paper\n",
    "    (https://arxiv.org/pdf/1712.05526.pdf).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dims = (64, 64, 3)\n",
    "        self.alpha = 0.8\n",
    "        self.image_path = BASE_FOLDER_PATH + 'hello_kitty.jpg'\n",
    "        self.crafted_trigger = self.create_trigger()\n",
    "\n",
    "    def create_trigger(self):\n",
    "        \"\"\"Prepare the trigger for blended attack.\"\"\"\n",
    "        \n",
    "        # Load kitty\n",
    "        img = Image.open(self.image_path)\n",
    "\n",
    "        # Resize to dimensions\n",
    "        tmp = img.resize(self.dims[:-1])\n",
    "\n",
    "        if self.dims[2] == 1:\n",
    "            tmp = ImageOps.grayscale(tmp)\n",
    "\n",
    "        tmp = np.asarray(tmp)\n",
    "        # This is needed in case the image is grayscale (width x height) to\n",
    "        # Add the channel dimension\n",
    "        tmp = tmp.reshape((self.dims))\n",
    "\n",
    "        if patch_level:\n",
    "            pil_image = Image.fromarray(tmp)\n",
    "            resized_pil = pil_image.resize((badpatches_patch_size, badpatches_patch_size))\n",
    "            tmp = np.array(resized_pil)\n",
    "\n",
    "        trigger_array = tmp / 255\n",
    "\n",
    "        return trigger_array\n",
    "\n",
    "    def apply_trigger(self, img):\n",
    "        \"\"\"applies the trigger on the image.\"\"\"\n",
    "        \n",
    "        crafted_trigger_normalized = self.crafted_trigger\n",
    "        \n",
    "        if crafted_trigger_normalized.max() > 1:\n",
    "            crafted_trigger_normalized = crafted_trigger_normalized / 255.0\n",
    "            \n",
    "        # Ensure the input image is normalized to [0, 1]\n",
    "        if img.max() > 1:\n",
    "            img = img / 255.0\n",
    "\n",
    "        img = ((img * self.alpha) + (crafted_trigger_normalized * (1 - self.alpha)))\n",
    "\n",
    "        return img.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateWarpedTrigger:\n",
    "    \"\"\"\n",
    "    A class that generates a warped trigger using a distortion grid for backdoor attacks.\n",
    "    Compatible with TensorFlow.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_height):\n",
    "        \"\"\"\n",
    "        Initialize the warped trigger generator.\n",
    "        :param dataset: Dataset name (e.g., 'mnist', 'cifar10', etc.) for defining image dimensions.\n",
    "        :param s: Strength of the warping effect.\n",
    "        :param grid_rescale: Rescaling factor for the distortion grid.\n",
    "        \"\"\"\n",
    "\n",
    "        self.dims = (64, 64, 3)\n",
    "        self.s = 0.25\n",
    "        self.k = 2\n",
    "        self.input_height = input_height\n",
    "        self.grid_rescale = 1.0\n",
    "\n",
    "        # Initialize the identity grid and noise grid for warping\n",
    "        self.identity_grid, self.noise_grid = self.generate_main_grid()\n",
    "\n",
    "    def generate_main_grid(self):\n",
    "        \"\"\"\n",
    "        Generate the identity and noise grids for the warped trigger.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create coarse random noise grid\n",
    "        grid_noise = tf.random.uniform(shape=(1, self.k, self.k, 2), minval=- 1.0, maxval=1.0)\n",
    "        grid_noise = grid_noise / tf.reduce_mean(tf.abs(grid_noise))\n",
    "\n",
    "        # Upsample the coarse noise to match the input height and width\n",
    "        noise_grid = tf.image.resize(grid_noise, size=(self.input_height, self.input_height), method=\"bicubic\")\n",
    "        # Clamp values for stability\n",
    "        noise_grid = tf.clip_by_value(noise_grid, -1.0, 1.0)\n",
    "\n",
    "        # Create the identity grid\n",
    "        array1d = tf.linspace(-1.0, 1.0, self.input_height)\n",
    "        x, y = tf.meshgrid(array1d, array1d)\n",
    "        identity_grid = tf.stack([y, x], axis=- 1)\n",
    "        identity_grid = identity_grid[tf.newaxis, ...]  # Add batch dimension\n",
    "\n",
    "        return identity_grid, noise_grid\n",
    "\n",
    "    def _grid_sample(self, image, grid):\n",
    "        \"\"\"\n",
    "        TensorFlow implementation of grid sampling for image warping.\n",
    "        :param image: The input image tensor with shape (batch_size, height, width, channels).\n",
    "        :param grid: The grid tensor with shape (batch_size, height, width, 2).\n",
    "        :return: Warped image tensor.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, height, width, channels = image.shape\n",
    "\n",
    "        # Split grid into x and y components\n",
    "        grid_y, grid_x = tf.split(grid, 2, axis=- 1)\n",
    "\n",
    "        # Rescale normalized grid coordinates to image pixel indices\n",
    "        grid_x = tf.cast((grid_x + 1.0) * 0.5 * tf.cast(width - 1, tf.float32), tf.int32)\n",
    "        grid_y = tf.cast((grid_y + 1.0) * 0.5 * tf.cast(height - 1, tf.float32), tf.int32)\n",
    "\n",
    "        # Remove the last dimension of grid_x and grid_y to match batch_indices shape\n",
    "        # Shape: (batch_size, height, width)\n",
    "        grid_x = tf.squeeze(grid_x, axis=- 1)\n",
    "        # Shape: (batch_size, height, width)\n",
    "        grid_y = tf.squeeze(grid_y, axis=- 1)\n",
    "\n",
    "        # Create batch indices for gather_nd\n",
    "        # Shape: (batch_size, 1, 1)\n",
    "        batch_indices = tf.range(batch_size)[:, tf.newaxis, tf.newaxis]\n",
    "        # Shape: (batch_size, height, width)\n",
    "        batch_indices = tf.tile(batch_indices, [1, height, width])\n",
    "\n",
    "        # Clip grid indices to stay within image bounds\n",
    "        grid_x = tf.clip_by_value(grid_x, 0, width - 1)\n",
    "        grid_y = tf.clip_by_value(grid_y, 0, height - 1)\n",
    "\n",
    "        # Stack indices for gather_nd\n",
    "        indices = tf.stack([batch_indices, grid_y, grid_x], axis=- 1)\n",
    "\n",
    "        sampled_image = tf.gather_nd(image, indices)\n",
    "\n",
    "        return sampled_image\n",
    "\n",
    "    def poison(self, image):\n",
    "        \"\"\"\n",
    "        Apply a warping trigger to the image.\n",
    "        :param image: A NumPy array representing the input image.\n",
    "        :return: A NumPy array of the warped image.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ensure the input image is normalized\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "\n",
    "        # Expand dimensions to (batch_size, height, width, channels)\n",
    "        image_tensor = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "        \n",
    "        if len(image_tensor.shape) == 3:  # Add batch dimension if missing\n",
    "            image_tensor = tf.expand_dims(image_tensor, axis=0)\n",
    "\n",
    "        # Generate the warped grid\n",
    "        grid_temps = (self.identity_grid + self.s * self.noise_grid / self.input_height) * self.grid_rescale\n",
    "        grid_temps = tf.clip_by_value(grid_temps, -1.0, 1.0)\n",
    "\n",
    "        # Warp the image using TensorFlow's grid_sample equivalent\n",
    "        poisoned_image = self._grid_sample(image_tensor, grid_temps)\n",
    "\n",
    "        # Squeeze batch dimension and convert back to NumPy\n",
    "        poisoned_image = tf.squeeze(poisoned_image, axis=0).numpy()\n",
    "\n",
    "        return poisoned_image\n",
    "\n",
    "    def apply_trigger(self, img):\n",
    "        \"\"\"\n",
    "        Alias for the poison function for consistency with other trigger generators.\n",
    "        :param img: Input image as a NumPy array.\n",
    "        :return: Warped image as a NumPy array.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.poison(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFNe3RD19qe3"
   },
   "source": [
    "# Creating backdoor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1750841615896,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "BBMzxM7Y5Qki"
   },
   "outputs": [],
   "source": [
    "class BackdoorDataset:\n",
    "    \"\"\"\n",
    "    TensorFlow-compatible dataset for backdoor attacks, enabling poisoning of specific samples.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clean_data, clean_labels, trigger_obj, epsilon, train):\n",
    "        \"\"\"\n",
    "        Initialize the backdoor dataset.\n",
    "        :param clean_data: Original dataset images (NumPy array).\n",
    "        :param clean_labels: Original dataset labels (one-hot encoded NumPy array).\n",
    "        :param trigger_obj: Instance of the GenerateSquareTrigger class.\n",
    "        :param epsilon: Fraction of samples to poison (default: 0.08 or 8%).\n",
    "        :param target_label: The target label for poisoned samples.\n",
    "        :param train: Whether this dataset is for training or testing.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.clean_data = clean_data\n",
    "        self.clean_labels = clean_labels\n",
    "        self.trigger_obj = trigger_obj\n",
    "        self.epsilon = epsilon\n",
    "        self.target_label = 0\n",
    "        self.train = train\n",
    "\n",
    "        if train:\n",
    "            self.poisoned_data, self.poisoned_labels = self.get_train_set()\n",
    "        else:\n",
    "            self.poisoned_data, self.poisoned_labels = self.get_test_set()\n",
    "\n",
    "    def poison(self, img):\n",
    "        \"\"\"Poison an image by applying the trigger.\"\"\"\n",
    "        \n",
    "        if patch_level:\n",
    "            emp = EMPatches()\n",
    "            img_patches, indices = emp.extract_patches(\n",
    "                img, patchsize=badpatches_patch_size, overlap=0)\n",
    "\n",
    "            for index, patch in enumerate(img_patches):\n",
    "                img_patches[index] = self.trigger_obj.apply_trigger(patch)\n",
    "            poisoned_img = emp.merge_patches(img_patches, indices)\n",
    "        else:\n",
    "            poisoned_img = self.trigger_obj.apply_trigger(img)\n",
    "\n",
    "        return poisoned_img\n",
    "\n",
    "    def get_train_set(self):\n",
    "        \"\"\"Generate the poisoned training set.\"\"\"\n",
    "        \n",
    "        poisoned_data = np.copy(self.clean_data)\n",
    "        poisoned_labels = np.copy(self.clean_labels)\n",
    "\n",
    "        num_samples = self.clean_data.shape[0]\n",
    "        num_poisoned = int(self.epsilon * num_samples)\n",
    "        poisoned_indices = np.random.choice(num_samples, size=num_poisoned, replace=False)\n",
    "\n",
    "        for idx in poisoned_indices:\n",
    "            # Convert one-hot label to scalar\n",
    "            label_idx = np.argmax(self.clean_labels[idx])\n",
    "            poisoned_data[idx] = self.poison(self.clean_data[idx])\n",
    "            poisoned_labels[idx] = tf.one_hot(\n",
    "                self.target_label, depth=4).numpy()\n",
    "\n",
    "        return poisoned_data, poisoned_labels\n",
    "\n",
    "    def get_test_set(self):\n",
    "        \"\"\"Generate the poisoned test set.\"\"\"\n",
    "        \n",
    "        temp = deepcopy(self.clean_data)\n",
    "        poisoned_data = []\n",
    "        poisoned_labels = []\n",
    "\n",
    "        for idx in range(self.clean_data.shape[0]):\n",
    "            # Convert one-hot label to scalar\n",
    "            label_idx = np.argmax(self.clean_labels[idx])\n",
    "            \n",
    "            if label_idx != self.target_label:\n",
    "                poisoned_data.append(self.poison(temp[idx]))\n",
    "                poisoned_labels.append(self.clean_labels[idx])\n",
    "\n",
    "        return np.array(poisoned_data), np.array(poisoned_labels)\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.poisoned_data, self.poisoned_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgbO-SZ59qe4"
   },
   "source": [
    "# Attack evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1750841615903,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "ci5hdeSn5Qkj"
   },
   "outputs": [],
   "source": [
    "def calculate_ASR(model, test_data, test_labels):\n",
    "    \"\"\"\n",
    "    Calculate the Attack Success Rate (ASR) of the backdoored model.\n",
    "    \"\"\"\n",
    "    \n",
    "    target_label = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    non_source_total = 0\n",
    "    misclassifications = 0\n",
    "\n",
    "    # Get model predictions\n",
    "    predictions = model.predict(test_data, batch_size=128)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    original_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "    # Source-agnostic attack\n",
    "    for i in range(len(original_labels)):\n",
    "        if original_labels[i] != target_label:\n",
    "            total += 1\n",
    "            \n",
    "            if predicted_labels[i] == target_label:\n",
    "                correct += 1\n",
    "\n",
    "    attack_acc = (correct * 100.0) / total\n",
    "\n",
    "    return attack_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FoMExvQ5Qkj"
   },
   "source": [
    "# Backdoor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "executionInfo": {
     "elapsed": 94,
     "status": "error",
     "timestamp": 1750841616967,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "dv3_kyYz5Qkj",
    "outputId": "71e2786f-690f-45b0-ddd3-3a36ebce6a61"
   },
   "outputs": [],
   "source": [
    "for poisoning_rate in poisoning_rates:\n",
    "    accuracies = []\n",
    "    asrs = []\n",
    "    \n",
    "    for run_number in range(runs):\n",
    "        if trigger == 'square':\n",
    "            trigger_generator = GenerateSquareTrigger((square_trigger_size, square_trigger_size))\n",
    "            \n",
    "        if trigger == 'blend':\n",
    "            trigger_generator = GenerateBlendedTrigger()\n",
    "            \n",
    "        if trigger == 'warped':\n",
    "            trigger_generator = GenerateWarpedTrigger(input_height=badpatches_patch_size if patch_level else 32)\n",
    "\n",
    "        training_data = np.load(BASE_FOLDER_PATH + 'data_files/train_data_celeba_smiling_eyeglass.npy')\n",
    "        training_label = np.load(BASE_FOLDER_PATH + 'data_files/train_label_celeba_smiling_eyeglass.npy')\n",
    "        testing_data = np.load(BASE_FOLDER_PATH + 'data_files/test_data_celeba_smiling_eyeglass.npy')\n",
    "        testing_label = np.load(BASE_FOLDER_PATH + 'data_files/test_label_celeba_smiling_eyeglass.npy')\n",
    "        training_label = np.squeeze(training_label)\n",
    "        testing_label = np.squeeze(testing_label)\n",
    "\n",
    "        backdoor_training_dataset = BackdoorDataset(\n",
    "            clean_data=training_data,\n",
    "            clean_labels=tf.one_hot(training_label, depth=4).numpy(),\n",
    "            trigger_obj=trigger_generator,\n",
    "            epsilon=poisoning_rate,\n",
    "            train=True\n",
    "        )\n",
    "        poisoned_training_data, poisoned_training_label = backdoor_training_dataset.get_data()\n",
    "\n",
    "        backdoor_test_dataset = BackdoorDataset(\n",
    "            clean_data=testing_data,\n",
    "            clean_labels=tf.one_hot(testing_label, depth=4).numpy(),\n",
    "            trigger_obj=trigger_generator,\n",
    "            epsilon=poisoning_rate,\n",
    "            train=False\n",
    "        )\n",
    "        poisoned_testing_data, poisoned_testing_label = backdoor_test_dataset.get_data()\n",
    "\n",
    "        # 1-of-K encoding\n",
    "        training_label = tf.reshape(tf.one_hot(training_label, axis=1, depth=4, dtype=tf.float64), (len(training_label), 4)).numpy()\n",
    "        testing_label = tf.reshape(tf.one_hot(testing_label, axis=1, depth=4, dtype=tf.float64), (len(testing_label), 4)).numpy()\n",
    "\n",
    "        # Shuffling the training set\n",
    "        indices = tf.range(start=0, limit=tf.shape(training_data)[0], dtype=tf.int32)\n",
    "        shuffled_indices = tf.random.shuffle(indices)\n",
    "        training_data = tf.gather(training_data, shuffled_indices, axis=0)\n",
    "        training_label = tf.gather(training_label, shuffled_indices, axis=0)\n",
    "        poisoned_training_data = tf.gather(poisoned_training_data, shuffled_indices, axis=0)\n",
    "        poisoned_training_label = tf.gather(poisoned_training_label, shuffled_indices, axis=0)\n",
    "\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(poisoned_testing_data[42], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"{trigger.title()}\", size=30, pad=20)\n",
    "        plt.savefig(BASE_FOLDER_PATH + f'result_images/celeba_{trigger}_patchlevel-{patch_level}.pdf', bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        # Creating the model\n",
    "        model_input = tf.keras.Input(shape=(poisoned_training_data.shape[1], poisoned_training_data.shape[2], poisoned_training_data.shape[3]))\n",
    "        model_output = WideResnet(model_input, num_blocks=1, k=10, num_classes=4)\n",
    "\n",
    "        # Model Aggregation\n",
    "        model = tf.keras.Model(model_input, model_output)\n",
    "\n",
    "        # Model Compilation\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['categorical_accuracy']\n",
    "        )\n",
    "\n",
    "        # Callbacks\n",
    "        weights_dict = {}\n",
    "        weight_callback = tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end=lambda epoch,\n",
    "            logs: weights_dict.update({epoch: model.get_weights()})\n",
    "        )\n",
    "\n",
    "        z = []\n",
    "        testing_after_epoch = tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end=lambda epoch,\n",
    "            logs: z.append(model.evaluate(testing_data, testing_label, batch_size=1000, verbose=1))\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        model_name = f'celeba_{trigger}_{poisoning_rate}-poisonrate_{patch_size}-patchsize_{badpatches_patch_size}-badpatches_patch_size_patchlevel-{patch_level}_runnumber{run_number}.weights.h5'\n",
    "\n",
    "        if train:\n",
    "            model.fit(\n",
    "                poisoned_training_data,\n",
    "                poisoned_training_label,\n",
    "                batch_size=128,\n",
    "                epochs=training_epochs,\n",
    "                callbacks=[testing_after_epoch, weight_callback]\n",
    "            )\n",
    "            model.save_weights(BASE_FOLDER_PATH + f'model_files/{model_name}.weights.h5')\n",
    "        else:\n",
    "            model.load_weights(BASE_FOLDER_PATH + f'model_files/{model_name}.weights.h5')\n",
    "\n",
    "        accuracies.append(round(model.evaluate(testing_data, testing_label, batch_size=1000, verbose=1)[-1] * 100, 2))\n",
    "        asrs.append(round(calculate_ASR(model=model, test_data=poisoned_testing_data, test_labels=poisoned_testing_label), 2))\n",
    "\n",
    "    print(f\"Experiment setup: dataset: '{dataset}', trigger: '{trigger}', poisoning_rate: '{poisoning_rate}', patch_level: '{patch_level}', patch_size: '{patch_size}'\")\n",
    "    print(f\"Acc max: '{np.max(accuracies)}'\")\n",
    "    print(f\"Acc avg: '{np.average(accuracies)}'\")\n",
    "    print(f'Acc std: {np.std(accuracies)}')\n",
    "    print(f\"Asr max: '{np.max(asrs)}'\")\n",
    "    print(f\"Asr avg: '{np.average(asrs)}'\")\n",
    "    print(f'Asr std: {np.std(asrs)}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
