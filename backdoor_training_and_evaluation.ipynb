{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1750841614805,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "qMkNHM0g5Qke",
    "outputId": "8db0a19a-4c27-4e71-cec7-eb961e9eeb73"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install --upgrade pip\n",
    "# !pip install empatches\n",
    "# !pip install tensorflow\n",
    "# !pip install torch\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1750841614839,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "kmAdfc8GSnzY"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from empatches import EMPatches\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from random import random\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import traceback\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1750841615821,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "yQfrshqK120J"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_FOLDER_PATH = '/home/.../BadPatches/'\n",
    "os.makedirs(BASE_FOLDER_PATH + '/data_files', exist_ok=True)\n",
    "os.makedirs(BASE_FOLDER_PATH + '/model_files', exist_ok=True)\n",
    "os.makedirs(BASE_FOLDER_PATH + '/result_images', exist_ok=True)\n",
    "\n",
    "dataset = 'GTSRB'  # 'CIFAR-10' , 'GTSRB'\n",
    "trigger = 'square'  # 'square' , 'blend' , 'warped'\n",
    "patch_level = False  # False for image-level trigger\n",
    "\n",
    "if patch_level:\n",
    "    poisoning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.02, 0.06, 0.1]\n",
    "else:\n",
    "    poisoning_rates = [0.001, 0.005, 0.02, 0.06, 0.1]\n",
    "\n",
    "runs = 1  # How many times do you want to train the model with the configuration\n",
    "k_input = 16  # Number of patches each expert gets\n",
    "patch_size = 4  # Hyperparameter 'l'\n",
    "badpatches_patch_size = 4  # Size of the patch to which the trigger is applied in the case of BadPatches\n",
    "square_trigger_size = 2  # Size of trigger patches, remember that black square should be smaller than patch size for patch level\n",
    "\n",
    "train = True  # False if you already trained a model and just want to run validation, not that poisoning of images can be random due to shuffling, results might slightly differ from the training validation\n",
    "save_pruned_model = True\n",
    "\n",
    "pruning_rates = [0.1, 0.2, 0.3]\n",
    "\n",
    "training_epochs = 25\n",
    "fine_tuning_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-ycU9O-5Qkg"
   },
   "source": [
    "# Gating Routers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1750841615830,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "cHgmRT3H5Qkg"
   },
   "outputs": [],
   "source": [
    "class gate(tf.keras.layers.Layer):\n",
    "    def __init__(self, k, gating_kernel_size, strides=(1, 1), padding='valid',\n",
    "                 data_format='channels_last', gating_activation=None,\n",
    "                 gating_kernel_initializer=tf.keras.initializers.RandomNormal, **kwargs):\n",
    "        super(gate, self).__init__(**kwargs)\n",
    "        self.k = k\n",
    "        self.gating_kernel_size = gating_kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.data_format = data_format\n",
    "        self.gating_activation = tf.keras.activations.get(gating_activation)\n",
    "        self.gating_kernel_initializer = gating_kernel_initializer\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "            \n",
    "        else:\n",
    "            channel_axis = -1\n",
    "\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs should be defined. Found `None`.')\n",
    "\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        gating_kernel_shape = self.gating_kernel_size + (input_dim, 1)\n",
    "        self.gating_kernel = self.add_weight(shape=gating_kernel_shape,\n",
    "                                             initializer=self.gating_kernel_initializer,\n",
    "                                             name='gating_kernel')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        gating_outputs = tf.keras.backend.conv2d(inputs, self.gating_kernel,strides=self.strides,\n",
    "                                                 padding=self.padding, data_format=self.data_format)\n",
    "\n",
    "        gating_outputs = tf.transpose(gating_outputs, perm=(0, 3, 1, 2))\n",
    "        x = tf.shape(gating_outputs)[2]\n",
    "        y = tf.shape(gating_outputs)[3]\n",
    "        gating_outputs = tf.reshape(gating_outputs, (tf.shape(gating_outputs)[0], tf.shape(gating_outputs)[1], x * y))\n",
    "\n",
    "        gating_outputs = self.gating_activation(gating_outputs)\n",
    "        [values, indices] = tf.math.top_k(gating_outputs, k=self.k, sorted=False)\n",
    "        indices = tf.reshape(indices, (tf.shape(indices)[0] * tf.shape(indices)[1], tf.shape(indices)[2]))\n",
    "        values = tf.reshape(values, (tf.shape(values)[0] * tf.shape(values)[1], tf.shape(values)[2]))\n",
    "        batch_t, k_t = tf.unstack(tf.shape(indices), num=2)\n",
    "\n",
    "        n = tf.shape(gating_outputs)[2]\n",
    "\n",
    "        indices_flat = tf.reshape(indices, [-1]) + tf.math.floordiv(tf.range(batch_t * k_t), k_t) * n\n",
    "        ret_flat = tf.math.unsorted_segment_sum(tf.reshape(values, [-1]), indices_flat, batch_t * n)\n",
    "        ret_rsh = tf.reshape(ret_flat, [batch_t, n])\n",
    "        ret_rsh_3 = tf.reshape(ret_rsh, (tf.shape(gating_outputs)[0], tf.shape(gating_outputs)[1], tf.shape(gating_outputs)[2]))\n",
    "\n",
    "        new_gating_outputs = tf.reshape(ret_rsh_3, (tf.shape(ret_rsh_3)[0], tf.shape(ret_rsh_3)[1], x, y))\n",
    "        new_gating_outputs = tf.transpose(new_gating_outputs, perm=(0, 2, 3, 1))\n",
    "        new_gating_outputs = tf.repeat(new_gating_outputs, tf.shape(self.gating_kernel)[0] * tf.shape(self.gating_kernel)[1] * tf.shape(self.gating_kernel)[2], axis=3)\n",
    "        new_gating_outputs = tf.reshape(new_gating_outputs, (tf.shape(new_gating_outputs)[0], tf.shape(new_gating_outputs)[1], tf.shape(new_gating_outputs)[2], tf.shape(self.gating_kernel)[0], tf.shape(self.gating_kernel)[1], tf.shape(self.gating_kernel)[2]))\n",
    "        new_gating_outputs = tf.transpose(new_gating_outputs, perm=(0, 1, 3, 2, 4, 5))\n",
    "        new_gating_outputs = tf.reshape(new_gating_outputs, (tf.shape(new_gating_outputs)[0], tf.shape(new_gating_outputs)[1] * tf.shape(new_gating_outputs)[2], tf.shape(new_gating_outputs)[3] * tf.shape(new_gating_outputs)[4], tf.shape(new_gating_outputs)[5]))\n",
    "        outputs = inputs * new_gating_outputs\n",
    "\n",
    "        return outputs, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SERp0GMl5Qkh"
   },
   "source": [
    "# Wideresnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1750841615839,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "qMoWs8YN5Qkh"
   },
   "outputs": [],
   "source": [
    "initializer_gate = keras.initializers.RandomNormal(mean=0.0, stddev=0.0001)\n",
    "\n",
    "def WideResnetBlock(x, channels, strides, channel_mismatch=False):\n",
    "    identity = x\n",
    "\n",
    "    out = layers.BatchNormalization()(x)\n",
    "    out = layers.ReLU()(out)\n",
    "    out = layers.Conv2D(filters=channels, kernel_size=3, strides=strides, padding='same')(out)\n",
    "\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.ReLU()(out)\n",
    "    out = layers.Conv2D(filters=channels, kernel_size=3, strides=1, padding='same')(out)\n",
    "\n",
    "    if channel_mismatch is not False:\n",
    "        identity = layers.Conv2D(\n",
    "            filters=channels, kernel_size=1, strides=strides, padding='valid')(identity)\n",
    "\n",
    "    out = layers.Add()([identity, out])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def WideResnetGroup(x, num_blocks, channels, strides):\n",
    "    x = WideResnetBlock(x=x, channels=channels, strides=strides, channel_mismatch=True)\n",
    "\n",
    "    for _ in range(num_blocks - 1):\n",
    "        x = WideResnetBlock(x=x, channels=channels, strides=(1, 1))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def WideResnet(x, num_blocks, k, num_classes=10):\n",
    "    widths = [int(v * k) for v in (16, 32, 64)]\n",
    "\n",
    "    x = layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = WideResnetGroup(x, num_blocks, widths[0], strides=(1, 1))\n",
    "    x = WideResnetGroup(x, num_blocks, widths[1], strides=2)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=640, kernel_size=3, strides=int(patch_size / 2), padding='same')(x)\n",
    "\n",
    "    x_1, indices_1 = gate(k_input, (1, 1), (1, 1), gating_activation=tf.nn.softmax, gating_kernel_initializer=initializer_gate)(x)\n",
    "    x_2, indices_2 = gate(k_input, (1, 1), (1, 1), gating_activation=tf.nn.softmax, gating_kernel_initializer=initializer_gate)(x)\n",
    "    x_3, indices_3 = gate(k_input, (1, 1), (1, 1), gating_activation=tf.nn.softmax, gating_kernel_initializer=initializer_gate)(x)\n",
    "    x_4, indices_4 = gate(k_input, (1, 1), (1, 1), gating_activation=tf.nn.softmax, gating_kernel_initializer=initializer_gate)(x)\n",
    "\n",
    "    x_1 = layers.BatchNormalization()(x_1)\n",
    "    x_2 = layers.BatchNormalization()(x_2)\n",
    "    x_3 = layers.BatchNormalization()(x_3)\n",
    "    x_4 = layers.BatchNormalization()(x_4)\n",
    "\n",
    "    x_1 = layers.ReLU()(x_1)\n",
    "    x_2 = layers.ReLU()(x_2)\n",
    "    x_3 = layers.ReLU()(x_3)\n",
    "    x_4 = layers.ReLU()(x_4)\n",
    "\n",
    "    x_1 = layers.Conv2D(filters=160, kernel_size=1, strides=1, padding='same')(x_1)\n",
    "    x_2 = layers.Conv2D(filters=160, kernel_size=1, strides=1, padding='same')(x_2)\n",
    "    x_3 = layers.Conv2D(filters=160, kernel_size=1, strides=1, padding='same')(x_3)\n",
    "    x_4 = layers.Conv2D(filters=160, kernel_size=1, strides=1, padding='same')(x_4)\n",
    "\n",
    "    x = tf.keras.layers.concatenate([x_1, x_2, x_3, x_4])\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.AveragePooling2D((int(32 / patch_size), int(32 / patch_size)))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(units=num_classes, activation='softmax')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-d9cWBl5Qki"
   },
   "source": [
    "# Trigger generation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateSquareTrigger:\n",
    "    \"\"\"\n",
    "    A class that creates a random square pattern that is used as a trigger for an\n",
    "    image dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.dims = (32, 32, 3)\n",
    "        self.size = size\n",
    "        trigger = np.zeros(self.dims, dtype=np.float32)\n",
    "        self.crafted_trigger = self.create_trigger_square(trigger)\n",
    "\n",
    "        if size[0] > self.dims[0] or size[1] > self.dims[1]:\n",
    "            raise Exception(\n",
    "                \"The size of the trigger is too large for the dataset items.\")\n",
    "\n",
    "    def create_trigger_square(self, trigger):\n",
    "        \"\"\"Create a square trigger.\"\"\"\n",
    "        \n",
    "        base_x, base_y = (0, 0)\n",
    "        \n",
    "        for x in range(self.size[0]):\n",
    "            for y in range(self.size[1]):\n",
    "                trigger[base_x + x][base_y + y] = np.ones((self.dims[2]))\n",
    "\n",
    "        return trigger\n",
    "\n",
    "    def apply_trigger(self, img):\n",
    "        \"\"\"applies the trigger on the image.\"\"\"\n",
    "        \n",
    "        base_x, base_y = (0, 0)\n",
    "        \n",
    "        for x in range(self.size[0]):\n",
    "            for y in range(self.size[1]):\n",
    "                img[base_x + x][base_y + y] = self.crafted_trigger[base_x + x][base_y + y]\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1750841615876,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "3ySd1qQa5Qki"
   },
   "outputs": [],
   "source": [
    "class GenerateBlendedTrigger:\n",
    "    \"\"\"\n",
    "    A class that uses images of the same dimensions as the dataset as triggers\n",
    "    that will be blended with the clean images.\n",
    "\n",
    "    We will use a random pattern or a hello-kitty image as the original paper\n",
    "    (https://arxiv.org/pdf/1712.05526.pdf).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dims = (32, 32, 3)\n",
    "        self.alpha = 0.8\n",
    "        self.image_path = BASE_FOLDER_PATH + 'hello_kitty.jpg'\n",
    "        self.crafted_trigger = self.create_trigger()\n",
    "\n",
    "    def create_trigger(self):\n",
    "        \"\"\"Prepare the trigger for blended attack.\"\"\"\n",
    "        \n",
    "        # Load kitty\n",
    "        img = Image.open(self.image_path)\n",
    "\n",
    "        # Resize to dimensions\n",
    "        tmp = img.resize(self.dims[:-1])\n",
    "\n",
    "        if self.dims[2] == 1:\n",
    "            tmp = ImageOps.grayscale(tmp)\n",
    "\n",
    "        tmp = np.asarray(tmp)\n",
    "        # This is needed in case the image is grayscale (width x height) to\n",
    "        # Add the channel dimension\n",
    "        tmp = tmp.reshape((self.dims))\n",
    "\n",
    "        if patch_level:\n",
    "            pil_image = Image.fromarray(tmp)\n",
    "            resized_pil = pil_image.resize((badpatches_patch_size, badpatches_patch_size))\n",
    "            tmp = np.array(resized_pil)\n",
    "\n",
    "        trigger_array = tmp / 255\n",
    "\n",
    "        return trigger_array\n",
    "\n",
    "    def apply_trigger(self, img):\n",
    "        \"\"\"applies the trigger on the image.\"\"\"\n",
    "        \n",
    "        crafted_trigger_normalized = self.crafted_trigger\n",
    "        \n",
    "        if crafted_trigger_normalized.max() > 1:\n",
    "            crafted_trigger_normalized = crafted_trigger_normalized / 255.0\n",
    "        \n",
    "        # Ensure the input image is normalized to [0, 1]\n",
    "        if img.max() > 1:\n",
    "            img = img / 255.0\n",
    "\n",
    "        img = ((img * self.alpha) + (crafted_trigger_normalized * (1 - self.alpha)))\n",
    "\n",
    "        return img.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateWarpedTrigger:\n",
    "    \"\"\"\n",
    "    A class that generates a warped trigger using a distortion grid for backdoor attacks.\n",
    "    Compatible with TensorFlow.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_height):\n",
    "        \"\"\"\n",
    "        Initialize the warped trigger generator.\n",
    "        :param dataset: Dataset name (e.g., 'mnist', 'cifar10', etc.) for defining image dimensions.\n",
    "        :param s: Strength of the warping effect.\n",
    "        :param grid_rescale: Rescaling factor for the distortion grid.\n",
    "        \"\"\"\n",
    "\n",
    "        self.dims = (32, 32, 3)\n",
    "        self.s = 0.25\n",
    "        self.k = 2\n",
    "        self.input_height = input_height\n",
    "        self.grid_rescale = 1.0\n",
    "\n",
    "        # Initialize the identity grid and noise grid for warping\n",
    "        self.identity_grid, self.noise_grid = self.generate_main_grid()\n",
    "\n",
    "    def generate_main_grid(self):\n",
    "        \"\"\"\n",
    "        Generate the identity and noise grids for the warped trigger.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create coarse random noise grid\n",
    "        grid_noise = tf.random.uniform(shape=(1, self.k, self.k, 2), minval=- 1.0, maxval=1.0)\n",
    "        grid_noise = grid_noise / tf.reduce_mean(tf.abs(grid_noise))\n",
    "\n",
    "        # Upsample the coarse noise to match the input height and width\n",
    "        noise_grid = tf.image.resize(grid_noise, size=(self.input_height, self.input_height), method=\"bicubic\")\n",
    "        # Clamp values for stability\n",
    "        noise_grid = tf.clip_by_value(noise_grid, -1.0, 1.0)\n",
    "\n",
    "        # Create the identity grid\n",
    "        array1d = tf.linspace(-1.0, 1.0, self.input_height)\n",
    "        x, y = tf.meshgrid(array1d, array1d)\n",
    "        identity_grid = tf.stack([y, x], axis=- 1)\n",
    "        identity_grid = identity_grid[tf.newaxis, ...]  # Add batch dimension\n",
    "\n",
    "        return identity_grid, noise_grid\n",
    "\n",
    "    def _grid_sample(self, image, grid):\n",
    "        \"\"\"\n",
    "        TensorFlow implementation of grid sampling for image warping.\n",
    "        :param image: The input image tensor with shape (batch_size, height, width, channels).\n",
    "        :param grid: The grid tensor with shape (batch_size, height, width, 2).\n",
    "        :return: Warped image tensor.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, height, width, channels = image.shape\n",
    "\n",
    "        # Split grid into x and y components\n",
    "        grid_y, grid_x = tf.split(grid, 2, axis=- 1)\n",
    "\n",
    "        # Rescale normalized grid coordinates to image pixel indices\n",
    "        grid_x = tf.cast((grid_x + 1.0) * 0.5 * tf.cast(width - 1, tf.float32), tf.int32)\n",
    "        grid_y = tf.cast((grid_y + 1.0) * 0.5 * tf.cast(height - 1, tf.float32), tf.int32)\n",
    "\n",
    "        # Remove the last dimension of grid_x and grid_y to match batch_indices shape\n",
    "        # Shape: (batch_size, height, width)\n",
    "        grid_x = tf.squeeze(grid_x, axis=-1)\n",
    "        # Shape: (batch_size, height, width)\n",
    "        grid_y = tf.squeeze(grid_y, axis=-1)\n",
    "\n",
    "        # Create batch indices for gather_nd\n",
    "        # Shape: (batch_size, 1, 1)\n",
    "        batch_indices = tf.range(batch_size)[:, tf.newaxis, tf.newaxis]\n",
    "        # Shape: (batch_size, height, width)\n",
    "        batch_indices = tf.tile(batch_indices, [1, height, width])\n",
    "\n",
    "        # Clip grid indices to stay within image bounds\n",
    "        grid_x = tf.clip_by_value(grid_x, 0, width - 1)\n",
    "        grid_y = tf.clip_by_value(grid_y, 0, height - 1)\n",
    "\n",
    "        # Stack indices for gather_nd\n",
    "        indices = tf.stack([batch_indices, grid_y, grid_x], axis=- 1)\n",
    "\n",
    "        sampled_image = tf.gather_nd(image, indices)\n",
    "\n",
    "        return sampled_image\n",
    "\n",
    "    def poison(self, image):\n",
    "        \"\"\"\n",
    "        Apply a warping trigger to the image.\n",
    "        :param image: A NumPy array representing the input image.\n",
    "        :return: A NumPy array of the warped image.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ensure the input image is normalized\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "\n",
    "        # Expand dimensions to (batch_size, height, width, channels)\n",
    "        image_tensor = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "        \n",
    "        if len(image_tensor.shape) == 3:  # Add batch dimension if missing\n",
    "            image_tensor = tf.expand_dims(image_tensor, axis=0)\n",
    "\n",
    "        # Generate the warped grid\n",
    "        grid_temps = (self.identity_grid + self.s * self.noise_grid / self.input_height) * self.grid_rescale\n",
    "        grid_temps = tf.clip_by_value(grid_temps, -1.0, 1.0)\n",
    "\n",
    "        # Warp the image using TensorFlow's grid_sample equivalent\n",
    "        poisoned_image = self._grid_sample(image_tensor, grid_temps)\n",
    "\n",
    "        # Squeeze batch dimension and convert back to NumPy\n",
    "        poisoned_image = tf.squeeze(poisoned_image, axis=0).numpy()\n",
    "\n",
    "        return poisoned_image\n",
    "\n",
    "    def apply_trigger(self, img):\n",
    "        \"\"\"\n",
    "        Alias for the poison function for consistency with other trigger generators.\n",
    "        :param img: Input image as a NumPy array.\n",
    "        :return: Warped image as a NumPy array.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.poison(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFNe3RD19qe3"
   },
   "source": [
    "# Creating backdoor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1750841615896,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "BBMzxM7Y5Qki"
   },
   "outputs": [],
   "source": [
    "class BackdoorDataset:\n",
    "    \"\"\"\n",
    "    TensorFlow-compatible dataset for backdoor attacks, enabling poisoning of specific samples.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clean_data, clean_labels, trigger_obj, epsilon, train, cifar):\n",
    "        \"\"\"\n",
    "        Initialize the backdoor dataset.\n",
    "        :param clean_data: Original dataset images (NumPy array).\n",
    "        :param clean_labels: Original dataset labels (one-hot encoded NumPy array).\n",
    "        :param trigger_obj: Instance of the GenerateSquareTrigger class.\n",
    "        :param epsilon: Fraction of samples to poison (default: 0.08 or 8%).\n",
    "        :param target_label: The target label for poisoned samples.\n",
    "        :param train: Whether this dataset is for training or testing.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.clean_data = clean_data\n",
    "        self.clean_labels = clean_labels\n",
    "        self.trigger_obj = trigger_obj\n",
    "        self.epsilon = epsilon\n",
    "        self.target_label = 0\n",
    "        self.train = train\n",
    "        self.cifar = cifar\n",
    "\n",
    "        if train:\n",
    "            self.poisoned_data, self.poisoned_labels = self.get_train_set()\n",
    "        else:\n",
    "            self.poisoned_data, self.poisoned_labels = self.get_test_set()\n",
    "\n",
    "    def poison(self, img):\n",
    "        \"\"\"Poison an image by applying the trigger.\"\"\"\n",
    "        \n",
    "        if patch_level:\n",
    "            emp = EMPatches()\n",
    "            img_patches, indices = emp.extract_patches(\n",
    "                img, patchsize=badpatches_patch_size, overlap=0)\n",
    "\n",
    "            for index, patch in enumerate(img_patches):\n",
    "                img_patches[index] = self.trigger_obj.apply_trigger(patch)\n",
    "            poisoned_img = emp.merge_patches(img_patches, indices)\n",
    "        else:\n",
    "            poisoned_img = self.trigger_obj.apply_trigger(img)\n",
    "\n",
    "        return poisoned_img\n",
    "\n",
    "    def get_train_set(self):\n",
    "        \"\"\"Generate the poisoned training set.\"\"\"\n",
    "        \n",
    "        poisoned_data = np.copy(self.clean_data)\n",
    "\n",
    "        if isinstance(self.trigger_obj, GenerateBlendedTrigger) or isinstance(self.trigger_obj, GenerateWarpedTrigger):\n",
    "            poisoned_data = poisoned_data / 255  # Apply normalization\n",
    "\n",
    "        poisoned_labels = np.copy(self.clean_labels)\n",
    "\n",
    "        num_samples = self.clean_data.shape[0]\n",
    "        num_poisoned = int(self.epsilon * num_samples)\n",
    "        poisoned_indices = np.random.choice(num_samples, size=num_poisoned, replace=False)\n",
    "\n",
    "        for idx in poisoned_indices:\n",
    "            # Convert one-hot label to scalar\n",
    "            label_idx = np.argmax(self.clean_labels[idx])\n",
    "            # Poison data and always change the label to target label\n",
    "            poisoned_data[idx] = self.poison(self.clean_data[idx])\n",
    "\n",
    "            if self.cifar is True:\n",
    "                poisoned_labels[idx] = tf.one_hot(self.target_label, depth=10).numpy()\n",
    "            else:\n",
    "                poisoned_labels[idx] = tf.one_hot(self.target_label, depth=43).numpy()\n",
    "\n",
    "        return poisoned_data, poisoned_labels\n",
    "\n",
    "    def get_test_set(self):\n",
    "        \"\"\"Generate the poisoned test set.\"\"\"\n",
    "        \n",
    "        temp = deepcopy(self.clean_data)\n",
    "        poisoned_data = []\n",
    "        poisoned_labels = []\n",
    "\n",
    "        for idx in range(self.clean_data.shape[0]):\n",
    "            # Convert one-hot label to scalar\n",
    "            label_idx = np.argmax(self.clean_labels[idx])\n",
    "            \n",
    "            if label_idx != self.target_label:\n",
    "                poisoned_data.append(self.poison(temp[idx]))\n",
    "                poisoned_labels.append(self.clean_labels[idx])\n",
    "\n",
    "        return np.array(poisoned_data), np.array(poisoned_labels)\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.poisoned_data, self.poisoned_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgbO-SZ59qe4"
   },
   "source": [
    "# Attack evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1750841615903,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "ci5hdeSn5Qkj"
   },
   "outputs": [],
   "source": [
    "def calculate_ASR(model, test_data, test_labels):\n",
    "    \"\"\"\n",
    "    Calculate the Attack Success Rate (ASR) of the backdoored model.\n",
    "    \"\"\"\n",
    "    \n",
    "    target_label = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    non_source_total = 0\n",
    "    misclassifications = 0\n",
    "\n",
    "    # Get model predictions\n",
    "    predictions = model.predict(test_data, batch_size=128)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    original_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "    # Source-agnostic attack\n",
    "    for i in range(len(original_labels)):\n",
    "        if original_labels[i] != target_label:\n",
    "            total += 1\n",
    "            \n",
    "            if predicted_labels[i] == target_label:\n",
    "                correct += 1\n",
    "\n",
    "    attack_acc = (correct * 100.0) / total\n",
    "\n",
    "    return attack_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FoMExvQ5Qkj"
   },
   "source": [
    "# Backdoor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "executionInfo": {
     "elapsed": 94,
     "status": "error",
     "timestamp": 1750841616967,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "dv3_kyYz5Qkj",
    "outputId": "71e2786f-690f-45b0-ddd3-3a36ebce6a61"
   },
   "outputs": [],
   "source": [
    "for poisoning_rate in poisoning_rates:\n",
    "    accuracies = []\n",
    "    asrs = []\n",
    "    \n",
    "    for run_number in range(runs):\n",
    "        if trigger == 'square':\n",
    "            trigger_generator = GenerateSquareTrigger((square_trigger_size, square_trigger_size))\n",
    "            \n",
    "        if trigger == 'blend':\n",
    "            trigger_generator = GenerateBlendedTrigger()\n",
    "            \n",
    "        if trigger == 'warped':\n",
    "            trigger_generator = GenerateWarpedTrigger(input_height=badpatches_patch_size if patch_level else 32)\n",
    "\n",
    "        if dataset == 'CIFAR-10':\n",
    "            print(\"CIFAR-10 as dataset\")\n",
    "            training_data = np.load(BASE_FOLDER_PATH + 'data_files/cifar_10_train_data_sorted.npy')\n",
    "            training_label = np.load(BASE_FOLDER_PATH + 'data_files/cifar_10_train_label_sorted.npy')\n",
    "            testing_data = np.load(BASE_FOLDER_PATH + 'data_files/cifar_10_test_data_sorted.npy')\n",
    "            testing_label = np.load(BASE_FOLDER_PATH + 'data_files/cifar_10_test_label_sorted.npy')\n",
    "\n",
    "            backdoor_training_dataset = BackdoorDataset(\n",
    "                clean_data=training_data,\n",
    "                clean_labels=tf.one_hot(training_label, depth=10).numpy(),\n",
    "                trigger_obj=trigger_generator,\n",
    "                epsilon=poisoning_rate,\n",
    "                train=True,\n",
    "                cifar=True\n",
    "            )\n",
    "            poisoned_training_data, poisoned_training_label = backdoor_training_dataset.get_data()\n",
    "\n",
    "            backdoor_test_dataset = BackdoorDataset(\n",
    "                clean_data=testing_data,\n",
    "                clean_labels=tf.one_hot(testing_label, depth=10).numpy(),\n",
    "                trigger_obj=trigger_generator,\n",
    "                epsilon=poisoning_rate,\n",
    "                train=False,\n",
    "                cifar=True\n",
    "            )\n",
    "            poisoned_testing_data, poisoned_testing_label = backdoor_test_dataset.get_data()\n",
    "\n",
    "            # 1-of-K encoding\n",
    "            training_label = tf.reshape(tf.one_hot(training_label, axis=1, depth=10, dtype=tf.float64), (len(training_label), 10)).numpy()\n",
    "            testing_label = tf.reshape(tf.one_hot(testing_label, axis=1, depth=10, dtype=tf.float64), (len(testing_label), 10)).numpy()\n",
    "\n",
    "        if dataset == 'GTSRB':\n",
    "            print(\"GTSRB as dataset\")\n",
    "            training_data = np.load(BASE_FOLDER_PATH + 'data_files/gtsrb_train_data_sorted.npy')\n",
    "            training_label = np.load(BASE_FOLDER_PATH + 'data_files/gtsrb_train_label_sorted.npy')\n",
    "            testing_data = np.load(BASE_FOLDER_PATH + 'data_files/gtsrb_test_data_sorted.npy')\n",
    "            testing_label = np.load(BASE_FOLDER_PATH + 'data_files/gtsrb_test_label_sorted.npy')\n",
    "\n",
    "            backdoor_training_dataset = BackdoorDataset(\n",
    "                clean_data=training_data,\n",
    "                clean_labels=tf.one_hot(training_label, depth=43).numpy(),\n",
    "                trigger_obj=trigger_generator,\n",
    "                epsilon=poisoning_rate,\n",
    "                train=True,\n",
    "                cifar=False\n",
    "            )\n",
    "            poisoned_training_data, poisoned_training_label = backdoor_training_dataset.get_data()\n",
    "\n",
    "            backdoor_test_dataset = BackdoorDataset(\n",
    "                clean_data=testing_data,\n",
    "                clean_labels=tf.one_hot(testing_label, depth=43).numpy(),\n",
    "                trigger_obj=trigger_generator,\n",
    "                epsilon=poisoning_rate,\n",
    "                train=False,\n",
    "                cifar=False\n",
    "            )\n",
    "            poisoned_testing_data, poisoned_testing_label = backdoor_test_dataset.get_data()\n",
    "\n",
    "            training_label = tf.reshape(tf.one_hot(training_label, depth=43, axis=1, dtype=tf.float64), (len(training_label), 43)).numpy()\n",
    "            testing_label = tf.reshape(tf.one_hot(testing_label, depth=43, axis=1, dtype=tf.float64), (len(testing_label), 43)).numpy()\n",
    "\n",
    "        # Shuffling the training set\n",
    "        indices = tf.range(start=0, limit=tf.shape(\n",
    "            training_data)[0], dtype=tf.int32)\n",
    "        shuffled_indices = tf.random.shuffle(indices)\n",
    "\n",
    "        training_data = tf.gather(training_data, shuffled_indices, axis=0)\n",
    "        training_label = tf.gather(training_label, shuffled_indices, axis=0)\n",
    "        poisoned_training_data = tf.gather(poisoned_training_data, shuffled_indices, axis=0)\n",
    "        poisoned_training_label = tf.gather(poisoned_training_label, shuffled_indices, axis=0)\n",
    "\n",
    "        # Normalizing and reshaping data\n",
    "        if isinstance(backdoor_training_dataset.trigger_obj, GenerateSquareTrigger):\n",
    "            poisoned_training_data = poisoned_training_data / 255\n",
    "            poisoned_testing_data = poisoned_testing_data / 255\n",
    "\n",
    "        training_data = training_data / 255\n",
    "        training_data = tf.cast(training_data, dtype=tf.dtypes.float32)\n",
    "        poisoned_training_data = tf.cast(poisoned_training_data, dtype=tf.dtypes.float32)\n",
    "        poisoned_testing_data = tf.cast(poisoned_testing_data, dtype=tf.dtypes.float32)\n",
    "\n",
    "        testing_data = testing_data / 255\n",
    "        testing_data = tf.cast(testing_data, dtype=tf.dtypes.float32)\n",
    "\n",
    "        # Dog and 80 sign images in clean testing dataset\n",
    "        _index = 5536 if dataset == 'CIFAR-10' else 217\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(testing_data[_index], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Clean\", size=30, pad=20)\n",
    "        plt.savefig(BASE_FOLDER_PATH +\n",
    "                    f'result_images/dog_{dataset}_clean.pdf', bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        # Dog and 80 sign images in poisoned testing dataset\n",
    "        _index = 4536 if dataset == 'CIFAR-10' else 217\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(poisoned_testing_data[_index], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"{trigger.title()}\", size=30, pad=20)\n",
    "        plt.savefig(BASE_FOLDER_PATH + f'result_images/{dataset}_{trigger}_patchlevel-{patch_level}.pdf', bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        # Creating the model\n",
    "        model_input = tf.keras.Input(shape=(\n",
    "            poisoned_training_data.shape[1], poisoned_training_data.shape[2], poisoned_training_data.shape[3]))\n",
    "        \n",
    "        if dataset == 'CIFAR-10':\n",
    "            model_output = WideResnet(model_input, num_blocks=1, k=10, num_classes=10)\n",
    "            \n",
    "        if dataset == 'GTSRB':\n",
    "            model_output = WideResnet(model_input, num_blocks=1, k=10, num_classes=43)\n",
    "\n",
    "        # Model Aggregation\n",
    "        model = tf.keras.Model(model_input, model_output)\n",
    "\n",
    "        # Model Compilation\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['categorical_accuracy']\n",
    "        )\n",
    "\n",
    "        # Callbacks\n",
    "        weights_dict = {}\n",
    "        weight_callback = tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end=lambda epoch,\n",
    "            logs: weights_dict.update({epoch: model.get_weights()})\n",
    "        )\n",
    "\n",
    "        z = []\n",
    "        testing_after_epoch = tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end=lambda epoch,\n",
    "            logs: z.append(model.evaluate(testing_data, testing_label, batch_size=1000, verbose=1))\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        model_name = f'{dataset}_{trigger}_{poisoning_rate}-poisonrate_{patch_size}-patchsize_{badpatches_patch_size}-badpatches_patch_size_patchlevel-{patch_level}_runnumber{run_number}.weights.h5'\n",
    "\n",
    "        if train:\n",
    "            model.fit(\n",
    "                poisoned_training_data,\n",
    "                poisoned_training_label,\n",
    "                batch_size=128,\n",
    "                epochs=training_epochs,\n",
    "                callbacks=[testing_after_epoch, weight_callback]\n",
    "            )\n",
    "            model.save_weights(BASE_FOLDER_PATH + f'model_files/{model_name}.weights.h5')\n",
    "        else:\n",
    "            model.load_weights(BASE_FOLDER_PATH + f'model_files/{model_name}.weights.h5')\n",
    "\n",
    "        accuracies.append(round(model.evaluate(testing_data, testing_label, batch_size=1000, verbose=1)[-1] * 100, 2))\n",
    "        asrs.append(round(calculate_ASR(model=model, test_data=poisoned_testing_data, test_labels=poisoned_testing_label), 2))\n",
    "\n",
    "    print(f\"Experiment setup: dataset: '{dataset}', trigger: '{trigger}', poisoning_rate: '{poisoning_rate}', patch_level: '{patch_level}', patch_size: '{patch_size}'\")\n",
    "    print(f\"Acc max: '{np.max(accuracies)}'\")\n",
    "    print(f\"Acc avg: '{np.average(accuracies)}'\")\n",
    "    print(f'Acc std: {np.std(accuracies)}')\n",
    "    print(f\"Asr max: '{np.max(asrs)}'\")\n",
    "    print(f\"Asr avg: '{np.average(asrs)}'\")\n",
    "    print(f'Asr std: {np.std(asrs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [5536, 3108] if dataset == 'CIFAR-10' else [401, 217]\n",
    "\n",
    "for index in indices:\n",
    "    patch_assignments = []\n",
    "    intermediate_model = tf.keras.Model(\n",
    "        inputs=model.input,\n",
    "        outputs=[layer.output[1] for layer in model.layers if isinstance(layer, gate)]\n",
    "    )\n",
    "    patch_indices = intermediate_model.predict(np.expand_dims(testing_data[index], axis=0), batch_size=128)\n",
    "    patch_indices = np.array(patch_indices)\n",
    "    epoch_assignments = []  # To store assignments for this epoch\n",
    "    \n",
    "    for expert_idx, indices in enumerate(patch_indices):\n",
    "        flattened_indices = indices.flatten()\n",
    "        grid_coordinates = [(i // 8, i % 8) for i in flattened_indices]  # Convert to (row, col)\n",
    "        epoch_assignments.append({\n",
    "            \"expert\": expert_idx + 1,\n",
    "            \"grid_coordinates\": grid_coordinates\n",
    "        })\n",
    "\n",
    "    patch_assignments.append(epoch_assignments)\n",
    "\n",
    "    grid_tracking = {expert_id: np.zeros((8, 8), dtype=int) for expert_id in range(1, 5)}\n",
    "    \n",
    "    for epoch_assignments in patch_assignments:\n",
    "        for assignment in epoch_assignments:\n",
    "            expert_id = assignment[\"expert\"]\n",
    "            \n",
    "            for (row, col) in assignment[\"grid_coordinates\"]:\n",
    "                grid_tracking[expert_id][row, col] += 1\n",
    "\n",
    "    def visualize_expert_specialization(grid_tracking):\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "        axes[0].imshow(testing_data[index], cmap='gray')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        for expert_id, grid in grid_tracking.items():\n",
    "            ax = axes[expert_id]\n",
    "            ax.imshow(grid, cmap=\"Blues\", interpolation=\"nearest\")\n",
    "            ax.set_title(f\"Expert {expert_id}\", size=30, pad=15)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(BASE_FOLDER_PATH + f'result_images/patch_selection_{dataset}_{index}.pdf', bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    visualize_expert_specialization(grid_tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasPruning:\n",
    "    def __init__(self, x_train, y_train, model, layer_names, prune_rate):\n",
    "        \"\"\"\n",
    "        Prunes specific layers in a Keras model.\n",
    "\n",
    "        Args:\n",
    "            model (tf.keras.Model): The trained Keras model.\n",
    "            layer_names (list): List of names of layers to prune.\n",
    "            prune_rate (float): Fraction of filters to remove.\n",
    "            x_train (tf.data.Dataset): Dataset for computing activations.\n",
    "            y_train (tf.data.Dataset): Corresponding labels for dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.model = model\n",
    "        self.layer_names = layer_names  # List of layer names to prune\n",
    "        self.prune_rate = prune_rate\n",
    "\n",
    "    def get_layer_activations(self, layer_name):\n",
    "        \"\"\"\n",
    "        Runs the dataset through the model and collects activations of the target layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        activation_model = tf.keras.Model(\n",
    "            inputs=self.model.input,\n",
    "            outputs=self.model.get_layer(layer_name).output\n",
    "        )\n",
    "\n",
    "        batch_size = 8\n",
    "        activations = []\n",
    "        \n",
    "        for i in range(0, len(self.x_train), batch_size):\n",
    "            batch = self.x_train[i:i + batch_size]\n",
    "            batch_activations = activation_model(batch, training=False)\n",
    "            activations.append(batch_activations)\n",
    "\n",
    "        return tf.concat(activations, axis=0)  # Shape: (num_samples, H, W, C)\n",
    "\n",
    "    def prune(self):\n",
    "        \"\"\"\n",
    "        Prunes filters in the selected layers based on their average activation.\n",
    "        \"\"\"\n",
    "        \n",
    "        for layer_name in self.layer_names:\n",
    "            print(f\"Pruning layer: '{layer_name}'\")\n",
    "\n",
    "            # Get the layer\n",
    "            layer = self.model.get_layer(layer_name)\n",
    "\n",
    "            # Get activations for the layer\n",
    "            activations = self.get_layer_activations(layer_name)\n",
    "            mean_activations = tf.reduce_mean(activations, axis=[0, 1, 2])  # Shape: (C,)\n",
    "\n",
    "            # Sort filters by activation\n",
    "            num_filters = mean_activations.shape[0]\n",
    "            num_pruned_filters = int(num_filters * self.prune_rate)\n",
    "            sorted_indices = tf.argsort(mean_activations)[:num_pruned_filters]  # Least active filters\n",
    "\n",
    "            # Get layer weights\n",
    "            weights, biases = layer.get_weights()  # Weights shape: (H, W, C_in, C_out)\n",
    "\n",
    "            # Set pruned filters to zero\n",
    "            weights[:, :, :, sorted_indices.numpy()] = 0\n",
    "            biases[sorted_indices.numpy()] = 0\n",
    "\n",
    "            # Assign updated weights back to the layer\n",
    "            layer.set_weights([weights, biases])\n",
    "\n",
    "            print(f\"Pruned '{num_pruned_filters}/{num_filters}' filters in '{layer_name}'\")\n",
    "\n",
    "        return self.model  # Return pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_by_index(model, layer_type, index):\n",
    "    \"\"\"\n",
    "    Retrieves the correct layer name dynamically based on its type and order.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained model.\n",
    "        layer_type (tf.keras.layers.Layer): The type of layer to search for (e.g., tf.keras.layers.Conv2D).\n",
    "        index (int): The occurrence index of the layer (0-based).\n",
    "\n",
    "    Returns:\n",
    "        str: The dynamically assigned name of the layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    layers = [layer.name for layer in model.layers if isinstance(layer, layer_type)]\n",
    "\n",
    "    if index >= len(layers):\n",
    "        raise ValueError(f\"Model has only '{len(layers)}' layers of type '{layer_type}', but index '{index}' was requested.\")\n",
    "\n",
    "    return layers[index]  # Return the dynamic layer name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pruning_rate in pruning_rates:\n",
    "    asr = round(calculate_ASR(model=model, test_data=poisoned_testing_data, test_labels=poisoned_testing_label), 2)\n",
    "    acc = round(model.evaluate(testing_data, testing_label, batch_size=1000, verbose=1)[-1] * 100, 2)\n",
    "    print(f\"ASR before pruning: '{asr}%'\")\n",
    "    print(f\"Acc before pruning: '{acc}'\")\n",
    "\n",
    "    layer_names_to_prune = [\n",
    "        get_layer_by_index(model, tf.keras.layers.Conv2D, 8),\n",
    "        get_layer_by_index(model, tf.keras.layers.Conv2D, 9),\n",
    "        get_layer_by_index(model, tf.keras.layers.Conv2D, 10),\n",
    "        get_layer_by_index(model, tf.keras.layers.Conv2D, 11),\n",
    "    ]\n",
    "\n",
    "    print(f\"Selected layers to prune: '{layer_names_to_prune}'\")\n",
    "    pruner = KerasPruning(\n",
    "        x_train=training_data,\n",
    "        y_train=training_label,\n",
    "        model=model,\n",
    "        layer_names=layer_names_to_prune,\n",
    "        prune_rate=pruning_rate\n",
    "    )\n",
    "    pruned_model = pruner.prune()\n",
    "\n",
    "    asr = round(calculate_ASR(model=pruned_model, test_data=poisoned_testing_data, test_labels=poisoned_testing_label), 2)\n",
    "    acc = round(pruned_model.evaluate(testing_data, testing_label, batch_size=1000, verbose=1)[-1] * 100, 2)\n",
    "    print(f\"ASR before fine tuning: '{asr}%'\")\n",
    "    print(f\"Acc before fine tuning: '{acc}'\")\n",
    "\n",
    "    pruned_model.fit(\n",
    "        training_data,\n",
    "        training_label,\n",
    "        batch_size=128,\n",
    "        epochs=fine_tuning_epochs,\n",
    "        callbacks=[testing_after_epoch, weight_callback]\n",
    "    )\n",
    "\n",
    "    asr = round(calculate_ASR(model=pruned_model, test_data=poisoned_testing_data, test_labels=poisoned_testing_label), 2)\n",
    "    acc = round(pruned_model.evaluate(testing_data, testing_label, batch_size=1000, verbose=1)[-1] * 100, 2)\n",
    "    print(f\"ASR after fine tuning: '{asr}%'\")\n",
    "    print(f\"Acc after fine tuning: '{acc}'\")\n",
    "\n",
    "    pruned_model.save_weights(BASE_FOLDER_PATH + f'model_files/pruned_{model_name}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
